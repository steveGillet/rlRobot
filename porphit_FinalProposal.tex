\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{PorphIt: PPO Robot Manipulator Morphology Design}

\author{\IEEEauthorblockN{Steve Gillet}
\IEEEauthorblockA{\textit{University of Colorado Boulder Robotics} \\
\textit{Email: steve.gillet@colorado.edu}}
% Add more authors if team members are known, e.g.:
\and
\IEEEauthorblockN{Jay Vakil}
\IEEEauthorblockA{\textit{Email: Jay.Vakil@colorado.edu}}
% \and
% \IEEEauthorblockN{Team Member 3}
% \IEEEauthorblockA{\textit{Email: member3.email@example.com}}
}

\maketitle

\begin{abstract}
This paper aims to address the cost and overactuation of robotics used in manufacturing by leveraging reinforcement learning (RL) to optimize robot morphologies to specific use cases.
RL is the right tool for this job because it can optimize over the large search space of different robot morphologies, effectively exploring a large variety of iterations from all over that search space.
The core methodology involves using a PPO model to iterate on the different morphologies and MuJoCo to evaluate their effectiveness in simulation.
The evaluation signals will be successes, timing, and manipulation measures and these will be used to show the effectiveness of the PPO model at creating morphologies that are faster, more effective, and more efficient for particular tasks.
\end{abstract}

\begin{IEEEkeywords}
reinforcement learning, robot morphology, manipulator design
\end{IEEEkeywords}

\section{Introduction and Motivation}
High costs, lack of tailorability, and lack of retrofitting are some of the most cited problems for the adoption of the robotics in industrial settings \cite{mckinsey2019industrial}.
The most commonly used robotic manipulator in manufacturing is FANUC series of 6-DOF manipulators which cost from \$17,500 to \$400,000 depending on their capacity and customizations \cite{standardbots2025fanucprice}\cite{patentpc2025toprobotics}.
Meanwhile \cite{russo2021task} and \cite{he2019underactuated} show that most manipulators are overactuated, less cost-efficient, and less effective for most tasks they are used in.
We want to address this problem by creating an automation pipeline where you feed in a use case and you get out a robot specifically optimized for that use case.
By matching the robot to the task we can ensure that robot can perform that task and only the parts needed for that task are used, reducing cost and increasing efficiency.
Here we describe the process by which robot manipulator morphologies are optimized to particular tasks using a PPO agent.

We chose reinforcement learning because morphology design is an optimization problem over a complex, high-dimensional search space and RL can explore and learn optimal designs through trial-and-error in simulation \cite{sutton1988learning}.
We chose PPO because it is robust and on-policy and excels in continuous action spaces which is ideal for morphology design where there are continuous parameters like link lengths \cite{schulman2017proximal}.
This is more effective than imitation learning because imitation learning can only use morphologies that already exist severely limiting sample space and innovation \cite{delgado2022robotics}.
Supervised learning suffers from a very similar issue where labelled data would have to be provided which would be infeasible and there is no mechanism for exploration \cite{kober2013reinforcement}.

The core method is to use MuJoCo to use stable\_baselines3 PPO implementation to iterate on the design parameters (number of links, joint types (X, Y, Z hinge, and Z actuation), and link lengths), use MuJoCo to simulate that iteration on a simple task using RRT* and numerical optimization IK for the path planning, feed the results (success, time, manipulability measure) back into the PPO model.
The evaluation signals will be successes, time, and manipulability measure.
Hopefully the better morphologies will be able to do the tasks more successfully, efficiently, and quicker and then we can vary the tasks slightly to find better morphologies for particular tasks.
The tasks will be kept simple but varied in ways to test different types of movements like moving near the base, moving from near to far, different elevations, more linear movements.
You can imagine that a lot of pick and place tasks are mostly linear and so might be largely done by linear actuators more efficiently.

The research questions we seek to answer: Can PPO be used to generate more efficient morphologies? What are the best evaluation signals to use? How does varying evaluation signals yield different results (optimizing for time yields simpler manipulators while optimizing for manipulability measure yields more complex manipulators?)?

\section{Background and Related Work}
Most of the research in this area codesigns morphology and control of modular, atomic robots like \cite{bhatia2023reinforcement} which uses PPO, \cite{tjanaka2023co} which uses TD3, \cite{spielberg2025accelerated}, and \cite{kalimuthu2023} which uses PPO and A3C for configuration and control of a reconfigurable, modular robot.
More pertinent \cite{ding2024modular} uses DDQN to co-optimize morphology and control for modular robotic manipulators and \cite{luck2019data} which uses soft actor critic for morphology and control of quadraped robots to avoid evalutating performance in sim or real to save time.
Our method will focus on manipulators with simple controllers to allow us to focus on morphology design.
The method will use Behavioral Cloning (BC) as a baseline and contrast PPO and Dreamer.
PPO has been used effectively in some of the relevant research and allows for searching over the continuous space of manipulator parameters instead of the discrete space of different modules.
Dreamer uses a 'world model' to predict outcomes and plan ahead which improves sample efficiency which might be even more effective by reducing computation needs \cite{hafner2019dream}.

\section{Methods}
The idea is to use PPO to iteratively optimize robot manipulator morphology parameters like number of joints, joint types (hinge X/Y/Z or slide in Z), and link lengths through simulation in MuJoCo.
The agent will propose designs, evaluate them on tasks using IK and RRT* path planning, and update the policy based on rewards for task success, efficiency, and manipulability.

The algorithmic structure will follow a standard RL loop: At each episode the PPO agent samples a morphology configuration from its policy (parameterized as a Gaussian distribution over continuous link lengths and discrete joint types and numbers via Gumbel-Softmax for differentiability).
The configuration will be instantiated in a dynamically generated MuJoCo XML model, simulated for a pick-and-place task (ie. moving from start to goal positions with varying elevations and distances), and controlled using numerical IK (using SciPy's minimize with L-BFGS-B) for joint angles and OMPL's RRT* for path planning in joint space.
Rewards are then computed and the policy is updated.

The object is to maximize expected cumulative rewards, defined as $r = w_1 \cdot s + w_2 \cdot \frac{1}{t} - w_3 \cdot c + w_4 \cdot m$, where $s$ is task success (1 if goal reached within tolerance, else 0), $t$ is trajectory time, $c$ is morphological complexity in number of joints, $m$ is the manipulability measure (joint Jacobian determinant \cite{yoshikawa1985manipulability}), and $w_i$ are the tunable weights.
PPO minimizes the clipped surrogate loss:
\begin{align}
\mathcal{L}(\theta) &= \hat{\mathbb{E}}_t \left[ \min\left(r_t(\theta) \hat{A}_t, \max(1-\epsilon, \min(1+\epsilon, r_t(\theta))) \hat{A}_t\right) \right] \notag \\
&\quad + S[\pi_\theta](s_t) - \beta \cdot \mathbb{E}_t [(\hat{V}_\theta(s_t) - V_t^{\text{targ}})^2]
\end{align}
where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ is the probability ratio, $\hat{A}_t$ is the advantage estimated via Generalized Advantage Estimation (GAE) \cite{schulman2015high}, $\epsilon=0.2$ bounds the ratio for stability, $S$ is an entropy bonus, and $\beta$ balances the value loss \cite{schulman2017proximal}.
For Dreamer the world model consists of a Recurrent State Space Model (RSSM) with encoder-decoder architecture to predict latent states and rewards, allowing imagination rollouts for policy updates \cite{hafner2019dream}.

Architectures use multi-layer perceptrons from Stable Baselines3.
The policy network has 2 hidden layers of 64 units each with tanh activation functions for actor and critic.
In Dreamer the world model adds a GRU recurrent layer (256 units) for dynamics prediction.

Update rules involve collecting trajectories over $N$ episodes, computing advantages, and performing $K$ epochs of minibatch SGD on the PPO loss with a clip ratio of 0.2.
For Dreamer, updates alternate between model fitting and actor-critic optimization on imagined trajectories.

The baseline will involved BC trained on demonstrations from standard 6-DOF manipulators to imitate fixed morphologies to contrast with RL's exploration.
As an ablation we will test removing the manipulability term from the reward function to assess the impact on design dexterity versus simplicity.

\section{Experimental Setup}
% Content here

\section{Timeline and Milestones}
% Content here

\section{Risks and Mitigations}
% Content here

\section{Resources}
% Content here

\section{Ethics and Safety}
% Content here

\section{Evaluation Plan}
% Content here

\section{Expected Results}
% Content here

\section{Contributions and Roles}
% Content here

\bibliographystyle{IEEEtran}
\bibliography{references} % Assuming a references.bib file

\end{document}